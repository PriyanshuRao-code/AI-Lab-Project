{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PriyanshuRao-code/AI-Lab-Project/blob/main/team_24_supervised_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EiJQ_AWgqpHQ",
        "outputId": "b0b87337-a080-4b00-9eb4-05c68f9bb3da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Cloning the repository...\n",
            "Repository is ready to use at: /content/AI-Lab-Project\n"
          ]
        }
      ],
      "source": [
        "# Don't do anything here. It's just a setup.\n",
        "import os\n",
        "import sys\n",
        "\n",
        "repo_name = \"AI-Lab-Project\"\n",
        "repo_url = \"https://github.com/PriyanshuRao-code/AI-Lab-Project.git\"\n",
        "repo_path = f\"/content/{repo_name}\"\n",
        "\n",
        "if os.path.exists(repo_path):\n",
        "    print(\"Repository already exists at:\", repo_path)\n",
        "else:\n",
        "    print(\"ðŸš€ Cloning the repository...\")\n",
        "    os.system(f\"git clone {repo_url}\")\n",
        "\n",
        "os.chdir(repo_path)\n",
        "sys.path.append(repo_path)\n",
        "\n",
        "print(\"Repository is ready to use at:\", repo_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start coding from here."
      ],
      "metadata": {
        "id": "ke4ojrxeAhBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from collections import Counter\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "LtmFopdFHqiV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('24.csv')"
      ],
      "metadata": {
        "id": "YMurM9hXHrtk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install import-ipynb\n",
        "import import_ipynb\n",
        "from team_24_data_preprocessing import data_preprocessing\n",
        "\n",
        "df_final_train, df_final_valid, df_final_test = data_preprocessing(df)"
      ],
      "metadata": {
        "id": "XGqWg7O3IGTv",
        "outputId": "6e5232fc-631d-4f8f-b9ea-770bda66b3fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: import-ipynb in /usr/local/lib/python3.11/dist-packages (0.2)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.11/dist-packages (from import-ipynb) (7.34.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from import-ipynb) (5.10.4)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from IPython->import-ipynb) (75.2.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from IPython->import-ipynb) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from IPython->import-ipynb) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from IPython->import-ipynb) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from IPython->import-ipynb) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from IPython->import-ipynb) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from IPython->import-ipynb) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from IPython->import-ipynb) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from IPython->import-ipynb) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from IPython->import-ipynb) (4.9.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->import-ipynb) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->import-ipynb) (4.23.0)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.11/dist-packages (from nbformat->import-ipynb) (5.7.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->IPython->import-ipynb) (0.8.4)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (0.24.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat->import-ipynb) (4.3.7)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->IPython->import-ipynb) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython->import-ipynb) (0.2.13)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from referencing>=0.28.4->jsonschema>=2.6->nbformat->import-ipynb) (4.13.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from team_24 import log_epoch_losses"
      ],
      "metadata": {
        "id": "2EHnqobDDBNK",
        "outputId": "167bd943-abed-4623-c3c8-4b96d9300a40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'log_epoch_losses' from 'team_24' (unknown location)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-11c151a7e6f6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mteam_24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlog_epoch_losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'log_epoch_losses' from 'team_24' (unknown location)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def x_y_separation(df_train, df_valid, df_test, target_column=\"Hazardous\"):\n",
        "  X_train = df_train.drop(columns=[target_column])\n",
        "  y_train = df_train[target_column]\n",
        "\n",
        "  X_valid = df_valid.drop(columns=[target_column])\n",
        "  y_valid = df_valid[target_column]\n",
        "\n",
        "  X_test = df_test.drop(columns=[target_column])\n",
        "  y_test = df_test[target_column]\n",
        "\n",
        "  return X_train, y_train, X_valid, y_valid, X_test, y_test"
      ],
      "metadata": {
        "id": "wmZkTsyNGLQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Perceptron(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Perceptron, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 8)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(8, 1)  # Output Layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "HzDNdFKi5pYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_perceptron(df_train, df_valid, df_test, target_column = \"Hazardous\", num_epochs=2000, lr=0.0001):\n",
        "\n",
        "  # Each time it may have different accuracy, recall due to ->  Weight Initialization of Perceptron is Random\n",
        "  X_train, y_train, X_valid, y_valid, X_test, y_test = x_y_separation(df_train, df_valid, df_test)\n",
        "\n",
        "  X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
        "  X_valid_tensor = torch.tensor(X_valid.values, dtype=torch.float32)\n",
        "  X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
        "\n",
        "  y_train_tensor = torch.tensor(y_train.values.reshape(-1, 1), dtype=torch.float32)\n",
        "  y_valid_tensor = torch.tensor(y_valid.values.reshape(-1, 1), dtype=torch.float32)\n",
        "  y_test_tensor = torch.tensor(y_test.values.reshape(-1, 1), dtype=torch.float32)\n",
        "\n",
        "\n",
        "  perceptron_model = Perceptron(X_train_tensor.shape[1])\n",
        "\n",
        "  criterion = nn.BCEWithLogitsLoss()\n",
        "  optimizer = optim.SGD(perceptron_model.parameters(), lr=lr)\n",
        "  #losses = []\n",
        "\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = perceptron_model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    #losses.append(loss.item())\n",
        "\n",
        "\n",
        "    # if epoch % 10 == 0:\n",
        "    #   print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
        "\n",
        "  with torch.no_grad():\n",
        "    y_valid_pred_nn = perceptron_model(X_valid_tensor).sigmoid().round()\n",
        "    y_test_pred_nn = perceptron_model(X_test_tensor).sigmoid().round()\n",
        "\n",
        "    # Convert tensors to numpy\n",
        "    y_valid_pred_nn = y_valid_pred_nn.detach().numpy()\n",
        "    y_test_pred_nn = y_test_pred_nn.detach().numpy()\n",
        "\n",
        "    # Convert y_valid, y_test to numpy for evaluation\n",
        "    y_valid_numpy = y_valid_tensor.detach().numpy()\n",
        "    y_test_numpy = y_test_tensor.detach().numpy()\n",
        "  #os.makedirs(\"logs\", exist_ok=True)\n",
        "  #pd.DataFrame({\"epoch\": list(range(num_epochs)), \"loss\": losses}).to_csv(\"perceptron_losses.csv\", index=False)\n",
        "\n",
        "  return perceptron_model, y_valid_numpy, y_test_numpy, y_valid_pred_nn, y_test_pred_nn"
      ],
      "metadata": {
        "id": "knCexYXL0vz2"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_perceptron_losses(df_train, target_column=\"Hazardous\", num_epochs=2000, lr=0.0001):\n",
        "\n",
        "\n",
        "    # Reuse only training data\n",
        "    X_train, y_train, _, _, _, _ = x_y_separation(df_train, df_train, df_train)\n",
        "\n",
        "    X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train.values.reshape(-1, 1), dtype=torch.float32)\n",
        "\n",
        "    model = Perceptron(X_train_tensor.shape[1])\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "    losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_train_tensor)\n",
        "        loss = criterion(outputs, y_train_tensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(loss.item())\n",
        "\n",
        "    return losses\n"
      ],
      "metadata": {
        "id": "N9AmRfZSE63I"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_gaussian(df_train, df_valid, df_test, target_column=\"Hazardous\"):\n",
        "\n",
        "  X_train, y_train, X_valid, y_valid, X_test, y_test = x_y_separation(df_train, df_valid, df_test)\n",
        "\n",
        "  X_train_nb = np.array(X_train)\n",
        "  X_valid_nb = np.array(X_valid)\n",
        "  X_test_nb = np.array(X_test)\n",
        "\n",
        "  y_train_nb = np.array(y_train)\n",
        "  y_valid_nb = np.array(y_valid)\n",
        "  y_test_nb = np.array(y_test)\n",
        "\n",
        "  naiveBayes = GaussianNB()\n",
        "  naiveBayes.fit(X_train_nb, y_train_nb)\n",
        "\n",
        "  y_valid_pred_nb = naiveBayes.predict(X_valid_nb)\n",
        "  y_test_pred_nb = naiveBayes.predict(X_test_nb)\n",
        "\n",
        "  return naiveBayes, y_valid_nb, y_test_nb, y_valid_pred_nb, y_test_pred_nb"
      ],
      "metadata": {
        "id": "sMXoIp-c6b7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mahalanobis_distance(x, mean, inv_cov_matrix):\n",
        "    diff = x - mean\n",
        "    mahal_dist = np.sqrt(np.dot(np.dot(diff.T, inv_cov_matrix), diff))\n",
        "    return mahal_dist"
      ],
      "metadata": {
        "id": "vYhuLP47GKCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def knn_mahalanobis(X_train, y_train, X_test, K):\n",
        "    # mean_vector = np.mean(X_train, axis=0)  # Compute mean of training data\n",
        "    cov_matrix = np.cov(X_train.T)\n",
        "    inv_cov_matrix = np.linalg.pinv(cov_matrix)  # Compute pseudo-inverse for stability\n",
        "    y_pred = []\n",
        "\n",
        "    for x in X_test:\n",
        "        distances = [mahalanobis_distance(x, X_train[i], inv_cov_matrix) for i in range(len(X_train))]\n",
        "        k_nearest_indices = np.argsort(distances)[:K]\n",
        "        k_nearest_labels = y_train[k_nearest_indices]\n",
        "        most_common_label = Counter(k_nearest_labels).most_common(1)[0][0]\n",
        "        y_pred.append(most_common_label)\n",
        "\n",
        "    return np.array(y_pred)"
      ],
      "metadata": {
        "id": "I3tOlpMTGiYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_knn(df_train, df_valid, df_test, target_column=\"Hazardous\", k_values = [1, 3, 5, 7, 11]):\n",
        "\n",
        "  X_train, y_train, X_valid, y_valid, X_test, y_test = x_y_separation(df_train, df_valid, df_test)\n",
        "\n",
        "  X_train_knn = np.array(X_train)\n",
        "  X_valid_knn = np.array(X_valid)\n",
        "  X_test_knn = np.array(X_test)\n",
        "\n",
        "  y_train_knn = np.array(y_train)\n",
        "  y_valid_knn = np.array(y_valid)\n",
        "  y_test_knn = np.array(y_test)\n",
        "\n",
        "  accuracies = []\n",
        "  for k in k_values:\n",
        "    y_valid_pred = knn_mahalanobis(X_train_knn, y_train_knn, X_valid_knn, k)\n",
        "    acc = accuracy_score(y_valid_knn, y_valid_pred)\n",
        "    accuracies.append(acc)\n",
        "\n",
        "  # Find the best K\n",
        "  best_k = k_values[np.argmax(accuracies)]\n",
        "\n",
        "  mean_vector = np.mean(X_train_knn, axis=0)\n",
        "  cov_matrix = np.cov(X_train_knn.T)\n",
        "  inv_cov_matrix = np.linalg.pinv(cov_matrix)\n",
        "  knn_final = KNeighborsClassifier(n_neighbors=best_k, metric=\"mahalanobis\", metric_params={\"VI\": inv_cov_matrix})\n",
        "  knn_final.fit(X_train_knn, y_train_knn)\n",
        "\n",
        "  y_valid_pred_knn = knn_final.predict(X_valid_knn)\n",
        "  y_test_pred_knn = knn_final.predict(X_test_knn)\n",
        "\n",
        "  return knn_final, y_valid_knn, y_test_knn, y_valid_pred_knn, y_test_pred_knn"
      ],
      "metadata": {
        "id": "ax8KSE7ZBwRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_logistic(df_train, df_valid, df_test, target_column=\"Hazardous\"):\n",
        "\n",
        "  X_train, y_train, X_valid, y_valid, X_test, y_test = x_y_separation(df_train, df_valid, df_test)\n",
        "  logistic_reg = LogisticRegression(solver='sag',max_iter=10000)\n",
        "  logistic_reg.fit(X_train,y_train)\n",
        "  y_val_pred = logistic_reg.predict(X_valid)\n",
        "  y_test_pred = logistic_reg.predict(X_test)\n",
        "\n",
        "  return logistic_reg, y_valid, y_test, y_val_pred, y_test_pred"
      ],
      "metadata": {
        "id": "QFW4JNBSNlTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_svc(df_train, df_valid, df_test, target_column=\"Hazardous\"):\n",
        "\n",
        "  X_train, y_train, X_valid, y_valid, X_test, y_test = x_y_separation(df_train, df_valid, df_test)\n",
        "\n",
        "  model = SVC(kernel='linear', C=1.0) # You can adjust the C parameter here\n",
        "  model.fit(X_train, y_train)\n",
        "  y_test_pred = model.predict(X_test)\n",
        "  y_val_pred = model.predict(X_valid)\n",
        "\n",
        "  return model, y_valid, y_test, y_val_pred, y_test_pred"
      ],
      "metadata": {
        "id": "eyq38ygAOKhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_svc_rbf(df_train, df_valid, df_test, target_column=\"Hazardous\"):\n",
        "  X_train, y_train, X_valid, y_valid, X_test, y_test = x_y_separation(df_train, df_valid, df_test)\n",
        "\n",
        "  model = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
        "  model.fit(X_train, y_train)\n",
        "  y_test_pred = model.predict(X_test)\n",
        "  y_val_pred = model.predict(X_valid)\n",
        "\n",
        "  return model, y_valid, y_test, y_val_pred, y_test_pred"
      ],
      "metadata": {
        "id": "env6x3E_liG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_svc_poly(df_train, df_valid, df_test, target_column=\"Hazardous\"):\n",
        "  X_train, y_train, X_valid, y_valid, X_test, y_test = x_y_separation(df_train, df_valid, df_test)\n",
        "  model = SVC(kernel='poly', degree=3, C=1.0, gamma='scale')\n",
        "  model.fit(X_train, y_train)\n",
        "  y_test_pred = model.predict(X_test)\n",
        "  y_val_pred = model.predict(X_valid)\n",
        "\n",
        "  return model, y_valid, y_test, y_val_pred, y_test_pred"
      ],
      "metadata": {
        "id": "0vwD2ROWl7x6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def evaluate_model(y_true, y_pred, model_name=\"Model\"):\n",
        "#     print(f\"\\nEvaluation Metrics for {model_name}:\")\n",
        "#     cm = confusion_matrix(y_true, y_pred)\n",
        "#     plt.figure(figsize=(5, 4))\n",
        "#     sns.heatmap(cm, annot=True, fmt=\"d\", cmap='coolwarm', xticklabels=[\"Not Hazardous\", \"Hazardous\"], yticklabels=[\"Not Hazardous\", \"Hazardous\"])\n",
        "#     plt.xlabel(\"Predicted Label\")\n",
        "#     plt.ylabel(\"True Label\")\n",
        "#     plt.title(f\"{model_name} Confusion Matrix\")\n",
        "#     plt.show()\n",
        "\n",
        "#     accuracy = accuracy_score(y_true, y_pred)\n",
        "#     precision = precision_score(y_true, y_pred, zero_division=0)\n",
        "#     recall = recall_score(y_true, y_pred)\n",
        "#     f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "#     print(f\"\\n  Accuracy:  {accuracy:.4f}\")\n",
        "#     print(f\"  Precision: {precision:.4f}\")\n",
        "#     print(f\"  Recall:    {recall:.4f}\")\n",
        "#     print(f\"  F1-score:  {f1:.4f}\")\n",
        "\n",
        "#     return accuracy, precision, recall, f1"
      ],
      "metadata": {
        "id": "tBtb100zdA-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# _, y_valid_numpy, y_test_numpy, y_valid_pred_nn, y_test_pred_nn = train_model_perceptron(df_final_train, df_final_valid, df_final_test)\n",
        "\n",
        "# evaluate_model(y_valid_numpy, y_valid_pred_nn, \"Perceptron (Validation)\")\n",
        "# evaluate_model(y_test_numpy, y_test_pred_nn, \"Perceptron (Test)\")\n",
        "\n",
        "\n",
        "# _, y_valid_nb, y_test_nb, y_valid_pred_nb, y_test_pred_nb = train_model_gaussian(df_final_train, df_final_valid, df_final_test)\n",
        "\n",
        "# evaluate_model(y_valid_nb, y_valid_pred_nb, \"NaÃ¯ve Bayes (Validation)\")\n",
        "# evaluate_model(y_test_nb, y_test_pred_nb, \"NaÃ¯ve Bayes (Test)\")\n",
        "\n",
        "\n",
        "# _, y_valid_knn, y_test_knn, y_valid_pred_knn, y_test_pred_knn = train_model_knn(df_final_train, df_final_valid, df_final_test)\n",
        "# evaluate_model(y_valid_knn, y_valid_pred_knn, \"KNN with Mahalanobis (Validation)\")\n",
        "# evaluate_model(y_test_knn, y_test_pred_knn, \"KNN with Mahalanobis (Test)\")\n",
        "\n",
        "# _, y_valid_logistic, y_test_logistic, y_valid_pred_logistic, y_test_pred_logistic = train_model_logistic(df_final_train, df_final_valid, df_final_test)\n",
        "# evaluate_model(y_valid_logistic, y_valid_pred_logistic, \"Logistic Regression (Validation)\")\n",
        "# evaluate_model(y_test_logistic, y_test_pred_logistic, \"Logistic Regression (Test)\")\n",
        "\n",
        "# _, y_valid_svc, y_test_svc, y_valid_pred_svc, y_test_pred_svc = train_model_svc(df_final_train, df_final_valid, df_final_test)\n",
        "# evaluate_model(y_valid_svc, y_valid_pred_svc, \"Support Vector Machine (Validation)\")\n",
        "# evaluate_model(y_test_svc, y_test_pred_svc, \"Support Vector Machine (Test)\")\n"
      ],
      "metadata": {
        "id": "Phw6hTMvsBiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **KNN analysis for k-values**"
      ],
      "metadata": {
        "id": "OFWEUaIQLjEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train, y_train, X_valid, y_valid, X_test, y_test = x_y_separation(df_final_train, df_final_valid, df_final_test)\n",
        "\n",
        "# X_train_knn = np.array(X_train)\n",
        "# X_valid_knn = np.array(X_valid)\n",
        "# X_test_knn = np.array(X_test)\n",
        "\n",
        "# y_train_knn = np.array(y_train)\n",
        "# y_valid_knn = np.array(y_valid)\n",
        "# y_test_knn = np.array(y_test)\n",
        "\n",
        "# accuracies = []\n",
        "# k_values = [1, 3, 5, 7, 11]\n",
        "# for k in k_values:\n",
        "#     y_valid_pred = knn_mahalanobis(X_train_knn, y_train_knn, X_valid_knn, k)\n",
        "#     acc = accuracy_score(y_valid_knn, y_valid_pred)\n",
        "#     accuracies.append(acc)\n",
        "\n",
        "#   # Find the best K\n",
        "# best_k = k_values[np.argmax(accuracies)]\n",
        "\n",
        "# mean_vector = np.mean(X_train_knn, axis=0)\n",
        "# cov_matrix = np.cov(X_train_knn.T)\n",
        "# inv_cov_matrix = np.linalg.pinv(cov_matrix)\n",
        "# knn_final = KNeighborsClassifier(n_neighbors=best_k, metric=\"mahalanobis\", metric_params={\"VI\": inv_cov_matrix})\n",
        "# knn_final.fit(X_train_knn, y_train_knn)\n",
        "\n",
        "\n",
        "# plt.figure(figsize=(10, 5))\n",
        "# plt.plot(k_values, accuracies, marker='o', linestyle='-', color='blue', label=\"Validation Accuracy\")\n",
        "# plt.axvline(x=best_k, color='red', linestyle='--', label=\"Best K\")\n",
        "# plt.xlabel(\"Number of Neighbors (K)\")\n",
        "# plt.ylabel(\"Validation Accuracy\")\n",
        "# plt.title(\"KNN with Mahalanobis: Accuracy vs K\")\n",
        "# plt.legend()\n",
        "# plt.grid()\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "GcuqVgHr2FXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kn5kZE8c3fpY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}