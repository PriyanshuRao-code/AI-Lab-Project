{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQgl00yQ8OaZBzmPYii8c/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PriyanshuRao-code/AI-Lab-Project/blob/main/team_24_data_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TVbyxjtmp9hA",
        "outputId": "3114c558-5277-4a88-e7be-4c1a674e20dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Repository already exists at: /content/AI-Lab-Project\n",
            "Repository is ready to use at: /content/AI-Lab-Project\n"
          ]
        }
      ],
      "source": [
        "# Don't do anything here. It is just a setup.\n",
        "import os\n",
        "import sys\n",
        "\n",
        "repo_name = \"AI-Lab-Project\"\n",
        "repo_url = \"https://github.com/PriyanshuRao-code/AI-Lab-Project.git\"\n",
        "repo_path = f\"/content/{repo_name}\"\n",
        "\n",
        "if os.path.exists(repo_path):\n",
        "    print(\"Repository already exists at:\", repo_path)\n",
        "else:\n",
        "    print(\"ðŸš€ Cloning the repository...\")\n",
        "    os.system(f\"git clone {repo_url}\")\n",
        "\n",
        "os.chdir(repo_path)\n",
        "sys.path.append(repo_path)\n",
        "\n",
        "print(\"Repository is ready to use at:\", repo_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start your code from here"
      ],
      "metadata": {
        "id": "Q57cyNuL_uUt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import zscore\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "rpkn8eeUAHH9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_dataframe = pd.read_csv('24.csv')"
      ],
      "metadata": {
        "id": "lTfhW7smnger"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_or_standardize_dates(df_train, df_valid, df_test, exclude_columns=[], year_method=\"normalize\", month_method=\"normalize\"):\n",
        "    year_columns = ['Close Approach Year', 'Orbit Determination Year', 'Epoch Close Approach Year']\n",
        "    month_columns = ['Close Approach Month', 'Orbit Determination Month', 'Epoch Close Approach Month']\n",
        "\n",
        "    # Columns from exclude columns to be standardized or normalized\n",
        "    year_cols_present = [col for col in year_columns if col in df_train.columns and col in exclude_columns]\n",
        "    month_cols_present = [col for col in month_columns if col in df_train.columns and col in exclude_columns]\n",
        "\n",
        "    # Scalers for Year and Month\n",
        "    year_scaler = MinMaxScaler() if year_method == \"normalize\" else StandardScaler()\n",
        "    month_scaler = MinMaxScaler() if month_method == \"normalize\" else StandardScaler()\n",
        "\n",
        "    # Normalize/Standardize Year Columns\n",
        "    if year_cols_present:\n",
        "        df_train[year_cols_present] = year_scaler.fit_transform(df_train[year_cols_present])\n",
        "        df_test[year_cols_present] = year_scaler.transform(df_test[year_cols_present])\n",
        "        df_valid[year_cols_present] = year_scaler.transform(df_valid[year_cols_present])\n",
        "\n",
        "    # Normalize/Standardize Month Columns\n",
        "    if month_cols_present:\n",
        "        df_train[month_cols_present] = month_scaler.fit_transform(df_train[month_cols_present])\n",
        "        df_test[month_cols_present] = month_scaler.transform(df_test[month_cols_present])\n",
        "        df_valid[month_cols_present] = month_scaler.transform(df_valid[month_cols_present])\n",
        "\n",
        "    return df_train, df_valid, df_test"
      ],
      "metadata": {
        "id": "k9C3YCrlL6lA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_outliers_iqr(df_train, df_valid, df_test, exclude_columns=[]):\n",
        "\n",
        "  df_train_numeric = df_train.select_dtypes(include=['number']).select_dtypes(exclude=['bool'])\n",
        "  df_train_numeric = df_train_numeric.drop(columns=exclude_columns) # Excluding the columns to be excluded\n",
        "\n",
        "  df_train_non_numeric = df_train.select_dtypes(exclude=['number'])\n",
        "\n",
        "  Q1 = df_train_numeric.quantile(0.25)\n",
        "  Q3 = df_train_numeric.quantile(0.75)\n",
        "  IQR = Q3 - Q1\n",
        "\n",
        "  lower_bound = Q1 - 1.5 * IQR\n",
        "  upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "  # Removing outliers\n",
        "  df_iqr = df_train_numeric[~((df_train_numeric < lower_bound) | (df_train_numeric > upper_bound)).any(axis=1)]\n",
        "  df_train_cleaned = pd.concat([df_iqr, df_train[exclude_columns].loc[df_iqr.index], df_train_non_numeric.loc[df_iqr.index]], axis=1)\n",
        "\n",
        "  return df_train_cleaned, df_valid, df_test"
      ],
      "metadata": {
        "id": "zitP9in21c6O"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_outliers_zscore(df_train, df_valid, df_test, z_score_threshold = 3, exclude_columns=[]):\n",
        "\n",
        "  df_train_numeric = df_train.select_dtypes(include=['number']).select_dtypes(exclude=['bool'])\n",
        "  df_train_numeric = df_train_numeric.drop(columns=exclude_columns)  # Excluding the columns to be excluded\n",
        "  df_train_non_numeric = df_train.select_dtypes(exclude=['number'])\n",
        "\n",
        "  z_scores = df_train_numeric.apply(zscore)\n",
        "  df_z = df_train_numeric[(z_scores.abs() < z_score_threshold).all(axis=1)]  # Remove rows with Z-score >  z_score_threshold in any column\n",
        "  df_train_cleaned = pd.concat([df_z, df_train[exclude_columns].loc[df_z.index], df_train_non_numeric.loc[df_z.index]], axis=1)\n",
        "\n",
        "  return df_train_cleaned, df_valid, df_test"
      ],
      "metadata": {
        "id": "3riZW1Jo13LC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_highly_correlated(df_train, df_valid, df_test, target_col=\"Hazardous\", high_corr_threshold = 0.99):\n",
        "\n",
        "  df_train_numeric = df_train.select_dtypes(include=['number']).select_dtypes(exclude=['bool'])\n",
        "\n",
        "  target_series = None\n",
        "  if target_col in df_train_numeric.columns: # High correaltion with target column should not be dropped\n",
        "    target_series = df_train[target_col]\n",
        "    df_train_numeric = df_train_numeric.drop(columns=[target_col])\n",
        "\n",
        "  high_corr_pairs = set()\n",
        "  correlation_matrix = df_train_numeric.corr()\n",
        "\n",
        "  for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i): # Lower triangular matrix\n",
        "      if abs(correlation_matrix.iloc[i, j]) >= high_corr_threshold:\n",
        "        col1 = correlation_matrix.columns[i]\n",
        "        col2 = correlation_matrix.columns[j]\n",
        "        high_corr_pairs.add((col1, col2))\n",
        "\n",
        "  columns_to_drop = {col2 for col1, col2 in high_corr_pairs}\n",
        "\n",
        "  df_train_reduced = df_train.drop(columns=columns_to_drop)\n",
        "  df_valid_reduced = df_valid.drop(columns=columns_to_drop)\n",
        "  df_test_reduced = df_test.drop(columns=columns_to_drop)\n",
        "\n",
        "  if target_series is not None: # If present in df_numeric and is removed, has to be added again\n",
        "    df_train_reduced[target_col] = target_series\n",
        "\n",
        "  return df_train_reduced, df_valid_reduced, df_test_reduced"
      ],
      "metadata": {
        "id": "7HbGOrEl7PI2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_minmax(df_train, df_valid, df_test, exclude_columns=[]):\n",
        "  scaler = MinMaxScaler()\n",
        "\n",
        "  df_train_numeric = df_train.select_dtypes(include=['number']).select_dtypes(exclude=['bool'])\n",
        "  df_test_numeric = df_test.select_dtypes(include=['number']).select_dtypes(exclude=['bool'])\n",
        "  df_valid_numeric = df_valid.select_dtypes(include=['number']).select_dtypes(exclude=['bool'])\n",
        "\n",
        "  exclude_columns = list(exclude_columns)\n",
        "\n",
        "  # Ensure only present columns are used for exclusion\n",
        "  cols_to_exclude = df_train_numeric.columns.intersection(exclude_columns).tolist()\n",
        "  if cols_to_exclude:\n",
        "    target_train = df_train[cols_to_exclude]\n",
        "    df_train_numeric = df_train_numeric.drop(columns=cols_to_exclude)\n",
        "\n",
        "    target_test = df_test[cols_to_exclude]\n",
        "    df_test_numeric = df_test_numeric.drop(columns=cols_to_exclude)\n",
        "\n",
        "    target_valid = df_valid[exclude_columns]\n",
        "    df_valid_numeric = df_valid_numeric.drop(columns=cols_to_exclude)\n",
        "\n",
        "  # Normalization\n",
        "  df_train[df_train_numeric.columns] = scaler.fit_transform(df_train_numeric)\n",
        "  df_test[df_test_numeric.columns] = scaler.transform(df_test_numeric)\n",
        "  df_valid[df_valid_numeric.columns] = scaler.transform(df_valid_numeric)\n",
        "\n",
        "  # Again including the exclude_columns\n",
        "  if cols_to_exclude:\n",
        "    df_train[cols_to_exclude] = target_train\n",
        "    df_test[cols_to_exclude] = target_test\n",
        "    df_valid[cols_to_exclude] = target_valid\n",
        "\n",
        "  df_train, df_valid, df_test = normalize_or_standardize_dates(df_train, df_valid, df_test, exclude_columns=exclude_columns, year_method=\"normalize\", month_method=\"normalize\")\n",
        "\n",
        "  return df_train, df_valid, df_test"
      ],
      "metadata": {
        "id": "f9XVR_A--2Rh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def standardize_data(df_train, df_valid, df_test, exclude_columns=[]):\n",
        "  scaler = StandardScaler()\n",
        "\n",
        "  df_train_numeric = df_train.select_dtypes(include=['number']).select_dtypes(exclude=['bool'])\n",
        "  df_test_numeric = df_test.select_dtypes(include=['number']).select_dtypes(exclude=['bool'])\n",
        "  df_valid_numeric = df_valid.select_dtypes(include=['number']).select_dtypes(exclude=['bool'])\n",
        "\n",
        "  exclude_columns = list(exclude_columns)\n",
        "\n",
        "  # Ensure only present columns are used for exclusion\n",
        "  cols_to_exclude = df_train_numeric.columns.intersection(exclude_columns).tolist()\n",
        "  if cols_to_exclude:\n",
        "    target_train = df_train[cols_to_exclude]\n",
        "    df_train_numeric = df_train_numeric.drop(columns=cols_to_exclude)\n",
        "\n",
        "    target_test = df_test[cols_to_exclude]\n",
        "    df_test_numeric = df_test_numeric.drop(columns=cols_to_exclude)\n",
        "\n",
        "    target_valid = df_valid[exclude_columns]\n",
        "    df_valid_numeric = df_valid_numeric.drop(columns=cols_to_exclude)\n",
        "\n",
        "  # Standardization\n",
        "  df_train[df_train_numeric.columns] = scaler.fit_transform(df_train_numeric)\n",
        "  df_test[df_test_numeric.columns] = scaler.transform(df_test_numeric)\n",
        "  df_valid[df_valid_numeric.columns] = scaler.transform(df_valid_numeric)\n",
        "\n",
        "\n",
        "  # Again including the exclude_columns\n",
        "  if cols_to_exclude:\n",
        "    df_train[cols_to_exclude] = target_train\n",
        "    df_test[cols_to_exclude] = target_test\n",
        "    df_valid[cols_to_exclude] = target_valid\n",
        "\n",
        "  df_train, df_valid, df_test = normalize_or_standardize_dates(df_train, df_valid, df_test, exclude_columns=exclude_columns, year_method=\"standardize\", month_method=\"standardize\")\n",
        "\n",
        "  return df_train, df_valid, df_test"
      ],
      "metadata": {
        "id": "1TEW4GRMRsj2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def label_encode_categorical(df_train, df_valid, df_test):\n",
        "\n",
        "  categorical_cols = df_train.select_dtypes(include=['object']).columns\n",
        "  label_encoders = {}\n",
        "\n",
        "\n",
        "  for col in categorical_cols:\n",
        "    label_encoders[col] = LabelEncoder()\n",
        "    df_train[col] = label_encoders[col].fit_transform(df_train[col])\n",
        "\n",
        "    df_valid[col] = df_valid[col].map(lambda x: label_encoders[col].transform([x])[0] if x in label_encoders[col].classes_ else -1)\n",
        "    df_valid[col] = df_valid[col].map(lambda x: label_encoders[col].transform([x])[0] if x in label_encoders[col].classes_ else -1)\n",
        "\n",
        "  return df_train, df_valid, df_test"
      ],
      "metadata": {
        "id": "5v2O0_pQSnPG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_high_corr_features(df_train, df_valid, df_test, target_col=\"Hazardous\", top_n=3):\n",
        "  df_train_numeric = df_train.select_dtypes(include=['number']).select_dtypes(exclude=['bool'])\n",
        "\n",
        "  # If target column is not in numerical form\n",
        "  if target_col not in df_train_numeric.columns:\n",
        "      raise ValueError(f\"Target column '{target_col}' must be numeric and present in the dataset.\")\n",
        "\n",
        "  corr_values = df_train_numeric.corr()[target_col].abs().sort_values(ascending=False)\n",
        "  selected_features = corr_values.drop(index=target_col).head(top_n).index.tolist()\n",
        "\n",
        "  # print(f\"Selected features based on correlation with '{target_col}': {selected_features}\")\n",
        "\n",
        "  return df_train[selected_features], df_valid[selected_features],  df_test[selected_features]"
      ],
      "metadata": {
        "id": "MnF9g8lMT52q"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_pca(df_train, df_valid, df_test, n_components=3):\n",
        "  pca = PCA(n_components=n_components)\n",
        "\n",
        "  df_train_numeric = df_train.select_dtypes(include=['number'])\n",
        "  df_test_numeric = df_test.select_dtypes(include=['number'])\n",
        "  df_valid_numeric = df_valid.select_dtypes(include=['number'])\n",
        "\n",
        "  pca.fit(df_train_numeric)\n",
        "  df_train_pca = pca.transform(df_train_numeric)\n",
        "  df_test_pca = pca.transform(df_test_numeric)\n",
        "  df_valid_pca = pca.transform(df_valid_numeric)\n",
        "\n",
        "  pca_columns = [f'PC{i+1}' for i in range(n_components)]\n",
        "  df_train_pca = pd.DataFrame(df_train_pca, columns=pca_columns)\n",
        "  df_test_pca = pd.DataFrame(df_test_pca, columns=pca_columns)\n",
        "  df_valid_pca = pd.DataFrame(df_valid_pca, columns=pca_columns)\n",
        "\n",
        "  return df_train_pca, df_valid_pca, df_test_pca"
      ],
      "metadata": {
        "id": "ywuE6x4uwK6m"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def numeric_conversion(df_orig, one_hot_encode_month=False):\n",
        "  df = df_orig.copy()\n",
        "  added_columns=[]\n",
        "\n",
        "  # Dropping 'Equinox' and 'Orbiting Body'\n",
        "  df.drop(columns=['Equinox', 'Orbiting Body'], errors='ignore', inplace=True)\n",
        "\n",
        "  # Converting 'Close Approach Date' to datetime\n",
        "  df['Close Approach Date'] = pd.to_datetime(df['Close Approach Date'])\n",
        "  df['Close Approach Year'] = df['Close Approach Date'].dt.year\n",
        "  df['Close Approach Month'] = df['Close Approach Date'].dt.month\n",
        "  added_columns.append('Close Approach Year')\n",
        "  added_columns.append('Close Approach Month')\n",
        "\n",
        "  # Converting 'Orbit Determination Date' to datetime\n",
        "  df['Orbit Determination Date'] = pd.to_datetime(df['Orbit Determination Date'])\n",
        "  df['Orbit Determination Year'] = df['Orbit Determination Date'].dt.year\n",
        "  df['Orbit Determination Month'] = df['Orbit Determination Date'].dt.month\n",
        "  added_columns.append('Orbit Determination Year')\n",
        "  added_columns.append('Orbit Determination Month')\n",
        "\n",
        "\n",
        "  # Converting 'Epoch Date Close Approach' to datetime\n",
        "  df['Converted Date'] = df['Epoch Date Close Approach'].apply(\n",
        "    lambda x: datetime.utcfromtimestamp(x / 1000) if pd.notnull(x) else None\n",
        "  )\n",
        "\n",
        "  df['Epoch Close Approach Year'] = df['Converted Date'].dt.year\n",
        "  df['Epoch Close Approach Month'] = df['Converted Date'].dt.month\n",
        "  added_columns.append('Epoch Close Approach Year')\n",
        "  added_columns.append('Epoch Close Approach Month')\n",
        "\n",
        "\n",
        "  # Encoding 'Hazardous' column\n",
        "  df['Hazardous'] = df['Hazardous'].astype(int)\n",
        "\n",
        "  # One-Hot Encoding for months (if needed)\n",
        "  if one_hot_encode_month:\n",
        "    one_hot_cols= ['Close Approach Month', 'Epoch Close Approach Month', 'Orbit Determination Month']\n",
        "    df = pd.get_dummies(df, columns=one_hot_cols, prefix=['Close Approach Month no.', 'Epoch Close Approach Month no.', 'Orbit Determination Month no.'], dtype=int)\n",
        "\n",
        "    added_columns = [col for col in added_columns if col not in ['Close Approach Month', 'Epoch Close Approach Month', 'Orbit Determination Month']]\n",
        "    added_columns.extend([col for col in df.columns if any(prefix in col for prefix in ['Close Approach Month no.', 'Epoch Close Approach Month no.', 'Orbit Determination Month no.'] )])\n",
        "\n",
        "  df = df.drop(columns=[\"Converted Date\", \"Close Approach Date\", \"Epoch Date Close Approach\", \"Orbit Determination Date\"])\n",
        "  return df, added_columns"
      ],
      "metadata": {
        "id": "koAiTYCCgGyH"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_preprocessing(df):\n",
        "  encoded, added_columns = numeric_conversion(df)\n",
        "\n",
        "  df_train, df_temp = train_test_split(encoded, test_size=0.4, random_state=42)\n",
        "  df_valid, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\n",
        "  df_train.reset_index(drop=True, inplace=True)\n",
        "  df_valid.reset_index(drop=True, inplace=True)\n",
        "  df_test.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  no_outliers_train, no_outliers_valid, no_outliers_test = remove_outliers_zscore(df_train, df_valid, df_test, exclude_columns = added_columns + [\"Hazardous\"])\n",
        "  no_highly_correlated_train, no_highly_correlated_valid, no_highly_correlated_test = remove_highly_correlated(no_outliers_train, no_outliers_valid, no_outliers_test)\n",
        "  exclude_cols = [col for col in added_columns + [\"Hazardous\"] if col in no_highly_correlated_train.columns]\n",
        "  normal_train, normal_test, normal_valid = normalize_minmax(no_highly_correlated_train, no_highly_correlated_valid, no_highly_correlated_test, exclude_columns =exclude_cols)\n",
        "  return normal_train, normal_valid, normal_test"
      ],
      "metadata": {
        "id": "DaeDAw9s6EIO"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}