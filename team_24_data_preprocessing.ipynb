{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP/pJdZQAIYWte7BvjvrBI+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PriyanshuRao-code/AI-Lab-Project/blob/main/team_24_data_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TVbyxjtmp9hA",
        "outputId": "e2644adc-7821-49a0-a2ed-228afbabd667",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Repository already exists at: /content/AI-Lab-Project\n",
            "Repository is ready to use at: /content/AI-Lab-Project\n"
          ]
        }
      ],
      "source": [
        "# Don't do anything here. It is just a setup.\n",
        "import os\n",
        "import sys\n",
        "\n",
        "repo_name = \"AI-Lab-Project\"\n",
        "repo_url = \"https://github.com/PriyanshuRao-code/AI-Lab-Project.git\"\n",
        "repo_path = f\"/content/{repo_name}\"\n",
        "\n",
        "if os.path.exists(repo_path):\n",
        "    print(\"Repository already exists at:\", repo_path)\n",
        "else:\n",
        "    print(\"ðŸš€ Cloning the repository...\")\n",
        "    os.system(f\"git clone {repo_url}\")\n",
        "\n",
        "os.chdir(repo_path)\n",
        "sys.path.append(repo_path)\n",
        "\n",
        "print(\"Repository is ready to use at:\", repo_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start your code from here"
      ],
      "metadata": {
        "id": "Q57cyNuL_uUt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import zscore\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "rpkn8eeUAHH9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_dataframe = pd.read_csv('24.csv')"
      ],
      "metadata": {
        "id": "lTfhW7smnger"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_outliers_iqr(df_orig):\n",
        "  df = df_orig.copy()\n",
        "\n",
        "  df_numeric = df.select_dtypes(include=['number']).select_dtypes(exclude=['bool'])\n",
        "  df_non_numeric = df.select_dtypes(exclude=['number'])\n",
        "\n",
        "  Q1 = df_numeric.quantile(0.25)\n",
        "  Q3 = df_numeric.quantile(0.75)\n",
        "  IQR = Q3 - Q1\n",
        "\n",
        "  lower_bound = Q1 - 1.5 * IQR\n",
        "  upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "  # Removing outliers\n",
        "  df_iqr = df_numeric[~((df_numeric < lower_bound) | (df_numeric > upper_bound)).any(axis=1)]\n",
        "  df_cleaned = pd.concat([df_iqr, df_non_numeric.loc[df_iqr.index]], axis=1)\n",
        "\n",
        "  return df_cleaned"
      ],
      "metadata": {
        "id": "zitP9in21c6O"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_outliers_zscore(df_orig, z_score_threshold = 3):\n",
        "  df = df_orig.copy()\n",
        "\n",
        "  df_numeric = df.select_dtypes(include=['number']).select_dtypes(exclude=['bool'])\n",
        "  df_non_numeric = df.select_dtypes(exclude=['number'])\n",
        "\n",
        "  z_scores = df_numeric.apply(zscore)\n",
        "  df_z = df_numeric[(z_scores.abs() < z_score_threshold).all(axis=1)]  # Remove rows with Z-score >  z_score_threshold in any column\n",
        "  df_cleaned = pd.concat([df_z, df_non_numeric.loc[df_z.index]], axis=1)\n",
        "\n",
        "  return df_cleaned"
      ],
      "metadata": {
        "id": "3riZW1Jo13LC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_highly_correlated(df_cleaned_orig, target_col=\"Hazardous\", high_corr_threshold = 0.99):\n",
        "  df_cleaned = df_cleaned_orig.copy()\n",
        "\n",
        "  df_numeric = df_cleaned.select_dtypes(include=['number']).select_dtypes(exclude=['bool'])\n",
        "\n",
        "  target_series = None\n",
        "  if target_col in df_numeric.columns: # High correaltion with target column should not be dropped\n",
        "    target_series = df_cleaned[target_col]\n",
        "    df_numeric = df_numeric.drop(columns=[target_col])\n",
        "\n",
        "  high_corr_pairs = set()\n",
        "  correlation_matrix = df_numeric.corr()\n",
        "\n",
        "  for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i): # Lower triangular matrix\n",
        "      if abs(correlation_matrix.iloc[i, j]) >= high_corr_threshold:\n",
        "        col1 = correlation_matrix.columns[i]\n",
        "        col2 = correlation_matrix.columns[j]\n",
        "        high_corr_pairs.add((col1, col2))\n",
        "\n",
        "  columns_to_drop = {col2 for col1, col2 in high_corr_pairs}\n",
        "  df_reduced = df_cleaned.drop(columns=columns_to_drop)\n",
        "\n",
        "  if target_series is not None: # If present in df_numeric and is removed, has to be added again\n",
        "    df_reduced[target_col] = target_series\n",
        "\n",
        "  return df_reduced"
      ],
      "metadata": {
        "id": "7HbGOrEl7PI2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_minmax(df_train, df_test, df_valid):\n",
        "  scaler = MinMaxScaler()\n",
        "\n",
        "  df_train_numeric = df_train.select_dtypes(include=['number']).select_dtypes(exclude=['bool'])\n",
        "  df_test_numeric = df_test.select_dtypes(include=['number']).select_dtypes(exclude=['bool'])\n",
        "  df_valid_numeric = df_valid.select_dtypes(include=['number']).select_dtypes(exclude=['bool'])\n",
        "\n",
        "  df_train[df_train_numeric.columns] = scaler.fit_transform(df_train_numeric)\n",
        "  df_test[df_test_numeric.columns] = scaler.transform(df_test_numeric)\n",
        "  df_valid[df_valid_numeric.columns] = scaler.transform(df_valid_numeric)\n",
        "\n",
        "  return df_train, df_test, df_valid"
      ],
      "metadata": {
        "id": "f9XVR_A--2Rh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def standardize_data(df_train, df_test, df_valid):\n",
        "  scaler = StandardScaler()\n",
        "\n",
        "  df_train_numeric = df_train.select_dtypes(include=['number']).select_dtypes(exclude=['bool'])\n",
        "  df_test_numeric = df_test.select_dtypes(include=['number']).select_dtypes(exclude=['bool'])\n",
        "  df_valid_numeric = df_valid.select_dtypes(include=['number']).select_dtypes(exclude=['bool'])\n",
        "\n",
        "  df_train[df_train_numeric.columns] = scaler.fit_transform(df_train_numeric)\n",
        "  df_test[df_test_numeric.columns] = scaler.transform(df_test_numeric)\n",
        "  df_valid[df_valid_numeric.columns] = scaler.transform(df_valid_numeric)\n",
        "\n",
        "  return df_train, df_test, df_valid"
      ],
      "metadata": {
        "id": "1TEW4GRMRsj2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def label_encode_categorical(df_normalized):\n",
        "  df_normalized_copy = df_normalized.copy()\n",
        "\n",
        "  categorical_cols = df_normalized_copy.select_dtypes(include=['object']).columns\n",
        "  label_encoder = LabelEncoder()\n",
        "\n",
        "  for col in categorical_cols:\n",
        "    df_normalized_copy[col] = label_encoder.fit_transform(df_normalized_copy[col])\n",
        "\n",
        "  return df_normalized_copy"
      ],
      "metadata": {
        "id": "5v2O0_pQSnPG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_high_corr_features(df_train, df_test, df_valid, target_col=\"Hazardous\", top_n=3):\n",
        "  df_train_numeric = df_train.select_dtypes(include=['number']).select_dtypes(exclude=['bool'])\n",
        "\n",
        "  # If target column is not in numerical form\n",
        "  if target_col not in df_train_numeric.columns:\n",
        "      raise ValueError(f\"Target column '{target_col}' must be numeric and present in the dataset.\")\n",
        "\n",
        "  corr_values = df_train_numeric.corr()[target_col].abs().sort_values(ascending=False)\n",
        "  selected_features = corr_values.drop(index=target_col).head(top_n).index.tolist()\n",
        "\n",
        "  # print(f\"Selected features based on correlation with '{target_col}': {selected_features}\")\n",
        "\n",
        "  return df_train[selected_features], df_test[selected_features], df_valid[selected_features]"
      ],
      "metadata": {
        "id": "MnF9g8lMT52q"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_pca(df_train, df_test, df_valid, n_components=3):\n",
        "  pca = PCA(n_components=n_components)\n",
        "\n",
        "  df_train_numeric = df_train.select_dtypes(include=['number'])\n",
        "  df_test_numeric = df_test.select_dtypes(include=['number'])\n",
        "  df_valid_numeric = df_valid.select_dtypes(include=['number'])\n",
        "\n",
        "  pca.fit(df_train_numeric)\n",
        "  df_train_pca = pca.transform(df_train_numeric)\n",
        "  df_test_pca = pca.transform(df_test_numeric)\n",
        "  df_valid_pca = pca.transform(df_valid_numeric)\n",
        "\n",
        "  pca_columns = [f'PC{i+1}' for i in range(n_components)]\n",
        "  df_train_pca = pd.DataFrame(df_train_pca, columns=pca_columns)\n",
        "  df_test_pca = pd.DataFrame(df_test_pca, columns=pca_columns)\n",
        "  df_valid_pca = pd.DataFrame(df_valid_pca, columns=pca_columns)\n",
        "\n",
        "  return df_train_pca, df_test_pca, df_valid_pca"
      ],
      "metadata": {
        "id": "ywuE6x4uwK6m"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def numeric_conversion(df_orig, one_hot_encode_month=False):\n",
        "  df = df_orig.copy()\n",
        "\n",
        "  # Dropping 'Equinox' and 'Orbiting Body'\n",
        "  df.drop(columns=['Equinox', 'Orbiting Body'], errors='ignore', inplace=True)\n",
        "\n",
        "  # Converting 'Close Approach Date' to datetime\n",
        "  df['Close Approach Date'] = pd.to_datetime(df['Close Approach Date'])\n",
        "  df['Close Approach Year'] = df['Close Approach Date'].dt.year\n",
        "  df['Close Approach Month'] = df['Close Approach Date'].dt.month\n",
        "\n",
        "\n",
        "\n",
        "  # Converting 'Epoch Date Close Approach' to datetime\n",
        "  df['Converted Date'] = df['Epoch Date Close Approach'].apply(\n",
        "    lambda x: datetime.utcfromtimestamp(x / 1000) if pd.notnull(x) else None\n",
        "  )\n",
        "\n",
        "  df['Epoch Close Approach Year'] = df['Converted Date'].dt.year\n",
        "  df['Epoch Close Approach Month'] = df['Converted Date'].dt.month\n",
        "\n",
        "\n",
        "\n",
        "  # Encoding 'Hazardous' column\n",
        "  df['Hazardous'] = df['Hazardous'].astype(int)\n",
        "\n",
        "  # One-Hot Encoding for months (if needed)\n",
        "  if one_hot_encode_month:\n",
        "    df = pd.get_dummies(df, columns=['Close Approach Month', 'Epoch Close Approach Month'], prefix=['Close Approach Month no.', 'Epoch Close Approach Month no.'], dtype=int)\n",
        "\n",
        "  df = df.drop(columns=[\"Converted Date\"])\n",
        "  return df"
      ],
      "metadata": {
        "id": "koAiTYCCgGyH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_preprocessing(df):\n",
        "  no_outliers = remove_outliers_zscore(df)\n",
        "  no_highly_correlated = remove_highly_correlated(no_outliers)\n",
        "  encoded = numeric_conversion(no_highly_correlated)\n",
        "  df_train, df_temp = train_test_split(encoded, test_size=0.4, random_state=42)\n",
        "  df_valid, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\n",
        "  normal_train, normal_test, normal_valid = normalize_minmax(df_train, df_test, df_valid)\n",
        "  normal_train.reset_index(drop=True, inplace=True)\n",
        "  normal_valid.reset_index(drop=True, inplace=True)\n",
        "  normal_test.reset_index(drop=True, inplace=True)\n",
        "  return normal_train, normal_valid, normal_test"
      ],
      "metadata": {
        "id": "DaeDAw9s6EIO"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}