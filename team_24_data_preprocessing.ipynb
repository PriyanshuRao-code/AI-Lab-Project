{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOS2apBRGb0WDT5enRPRQAM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PriyanshuRao-code/AI-Lab-Project/blob/main/team_24_data_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TVbyxjtmp9hA",
        "outputId": "b919abad-a542-4465-b832-1a06acc7c061",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Cloning the repository...\n",
            "Repository is ready to use at: /content/AI-Lab-Project\n"
          ]
        }
      ],
      "source": [
        "# Don't do anything here. It is just a setup.\n",
        "import os\n",
        "import sys\n",
        "\n",
        "repo_name = \"AI-Lab-Project\"\n",
        "repo_url = \"https://github.com/PriyanshuRao-code/AI-Lab-Project.git\"\n",
        "repo_path = f\"/content/{repo_name}\"\n",
        "\n",
        "if os.path.exists(repo_path):\n",
        "    print(\"Repository already exists at:\", repo_path)\n",
        "else:\n",
        "    print(\"ðŸš€ Cloning the repository...\")\n",
        "    os.system(f\"git clone {repo_url}\")\n",
        "\n",
        "os.chdir(repo_path)\n",
        "sys.path.append(repo_path)\n",
        "\n",
        "print(\"Repository is ready to use at:\", repo_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start your code from here"
      ],
      "metadata": {
        "id": "Q57cyNuL_uUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import zscore\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "rpkn8eeUAHH9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_dataframe = pd.read_csv('24.csv')"
      ],
      "metadata": {
        "id": "lTfhW7smnger"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_outliers_iqr(df_train, df_valid, df_test):\n",
        "\n",
        "  df_train_numeric = df_train.select_dtypes(include=['number']).select_dtypes(exclude=['bool'])\n",
        "  df_train_non_numeric = df_train.select_dtypes(exclude=['number'])\n",
        "\n",
        "  Q1 = df_train_numeric.quantile(0.25)\n",
        "  Q3 = df_train_numeric.quantile(0.75)\n",
        "  IQR = Q3 - Q1\n",
        "\n",
        "  lower_bound = Q1 - 1.5 * IQR\n",
        "  upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "  # Removing outliers\n",
        "  df_iqr = df_train_numeric[~((df_train_numeric < lower_bound) | (df_train_numeric > upper_bound)).any(axis=1)]\n",
        "  df_train_cleaned = pd.concat([df_iqr, df_train_non_numeric.loc[df_iqr.index]], axis=1)\n",
        "\n",
        "  return df_train_cleaned, df_valid, df_test"
      ],
      "metadata": {
        "id": "zitP9in21c6O"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_outliers_zscore(df_train, df_valid, df_test, z_score_threshold = 3):\n",
        "\n",
        "  df_train_numeric = df_train.select_dtypes(include=['number']).select_dtypes(exclude=['bool'])\n",
        "  df_train_non_numeric = df_train.select_dtypes(exclude=['number'])\n",
        "\n",
        "  z_scores = df_train_numeric.apply(zscore)\n",
        "  df_z = df_train_numeric[(z_scores.abs() < z_score_threshold).all(axis=1)]  # Remove rows with Z-score >  z_score_threshold in any column\n",
        "  df_train_cleaned = pd.concat([df_z, df_train_non_numeric.loc[df_z.index]], axis=1)\n",
        "\n",
        "  return df_train_cleaned, df_valid, df_test"
      ],
      "metadata": {
        "id": "3riZW1Jo13LC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_highly_correlated(df_train, df_valid, df_test, high_corr_threshold = 0.99):\n",
        "\n",
        "  df_train_numeric = df_train.select_dtypes(include=['number']).select_dtypes(exclude=['bool'])\n",
        "\n",
        "  high_corr_pairs = set()\n",
        "  correlation_matrix = df_train_numeric.corr()\n",
        "\n",
        "  for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i): # Lower triangular matrix\n",
        "      if abs(correlation_matrix.iloc[i, j]) >= high_corr_threshold:\n",
        "        col1 = correlation_matrix.columns[i]\n",
        "        col2 = correlation_matrix.columns[j]\n",
        "        high_corr_pairs.add((col1, col2))\n",
        "\n",
        "  columns_to_drop = {col2 for col1, col2 in high_corr_pairs}\n",
        "\n",
        "  df_train_reduced = df_train.drop(columns=columns_to_drop)\n",
        "  df_valid_reduced = df_valid.drop(columns=columns_to_drop)\n",
        "  df_test_reduced = df_test.drop(columns=columns_to_drop)\n",
        "\n",
        "  return df_train_reduced, df_valid_reduced, df_test_reduced"
      ],
      "metadata": {
        "id": "7HbGOrEl7PI2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_minmax(df_train_orig, df_valid_orig, df_test_orig):\n",
        "  df_train = df_train_orig.copy()\n",
        "  df_valid = df_valid_orig.copy()\n",
        "  df_test = df_test_orig.copy()\n",
        "  scaler = MinMaxScaler()\n",
        "\n",
        "  df_train_numeric = df_train.select_dtypes(include=['number']).select_dtypes(exclude=['bool'])\n",
        "  df_test_numeric = df_test.select_dtypes(include=['number']).select_dtypes(exclude=['bool'])\n",
        "  df_valid_numeric = df_valid.select_dtypes(include=['number']).select_dtypes(exclude=['bool'])\n",
        "\n",
        "  # Normalization\n",
        "  df_train[df_train_numeric.columns] = scaler.fit_transform(df_train_numeric)\n",
        "  df_test[df_test_numeric.columns] = scaler.transform(df_test_numeric)\n",
        "  df_valid[df_valid_numeric.columns] = scaler.transform(df_valid_numeric)\n",
        "\n",
        "  return df_train, df_valid, df_test"
      ],
      "metadata": {
        "id": "f9XVR_A--2Rh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def standardize_data(df_train_orig, df_valid_orig, df_test_orig):\n",
        "  df_train = df_train_orig.copy()\n",
        "  df_valid = df_valid_orig.copy()\n",
        "  df_test = df_test_orig.copy()\n",
        "  scaler = StandardScaler()\n",
        "\n",
        "  df_train_numeric = df_train.select_dtypes(include=['number']).select_dtypes(exclude=['bool'])\n",
        "  df_test_numeric = df_test.select_dtypes(include=['number']).select_dtypes(exclude=['bool'])\n",
        "  df_valid_numeric = df_valid.select_dtypes(include=['number']).select_dtypes(exclude=['bool'])\n",
        "\n",
        "  # Standardization\n",
        "  df_train[df_train_numeric.columns] = scaler.fit_transform(df_train_numeric)\n",
        "  df_test[df_test_numeric.columns] = scaler.transform(df_test_numeric)\n",
        "  df_valid[df_valid_numeric.columns] = scaler.transform(df_valid_numeric)\n",
        "\n",
        "  return df_train, df_valid, df_test"
      ],
      "metadata": {
        "id": "1TEW4GRMRsj2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def label_encode_categorical(df_train, df_valid, df_test):\n",
        "\n",
        "  categorical_cols = df_train.select_dtypes(include=['object']).columns\n",
        "  label_encoders = {}\n",
        "\n",
        "  for col in categorical_cols:\n",
        "    label_encoders[col] = LabelEncoder()\n",
        "    df_train[col] = label_encoders[col].fit_transform(df_train[col])\n",
        "\n",
        "    df_valid[col] = df_valid[col].map(lambda x: label_encoders[col].transform([x])[0] if x in label_encoders[col].classes_ else -1)\n",
        "    df_test[col] = df_test[col].map(lambda x: label_encoders[col].transform([x])[0] if x in label_encoders[col].classes_ else -1)\n",
        "\n",
        "  df_valid = df_valid[df_train.columns]\n",
        "  df_test = df_test[df_train.columns]\n",
        "\n",
        "  return df_train, df_valid, df_test"
      ],
      "metadata": {
        "id": "5v2O0_pQSnPG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_high_corr_features(df_train, df_valid, df_test, target_col=\"Hazardous\", top_n=3):\n",
        "  df_train_numeric = df_train.select_dtypes(include=['number']).select_dtypes(exclude=['bool'])\n",
        "\n",
        "  corr_values = df_train_numeric.corr()[target_col].abs().sort_values(ascending=False)\n",
        "  selected_features = corr_values.drop(index=target_col).head(top_n).index.tolist()\n",
        "\n",
        "  # print(f\"Selected features based on correlation with '{target_col}': {selected_features}\")\n",
        "\n",
        "  return df_train[selected_features], df_valid[selected_features], df_test[selected_features]"
      ],
      "metadata": {
        "id": "MnF9g8lMT52q"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_pca(df_train, df_valid, df_test, target_col=\"Hazardous\", n_components=11):\n",
        "\n",
        "  # Split features & target\n",
        "  X_train = df_train.drop(columns=[target_col])\n",
        "  X_valid = df_valid.drop(columns=[target_col])\n",
        "  X_test  = df_test.drop(columns=[target_col])\n",
        "\n",
        "  y_train = df_train[target_col].reset_index(drop=True)\n",
        "  y_valid = df_valid[target_col].reset_index(drop=True)\n",
        "  y_test  = df_test[target_col].reset_index(drop=True)\n",
        "\n",
        "  # PCA\n",
        "  pca = PCA(n_components=n_components)\n",
        "  pca.fit(X_train)\n",
        "\n",
        "  X_train_pca = pd.DataFrame(pca.transform(X_train), columns=[f'PC{i+1}' for i in range(n_components)])\n",
        "  X_valid_pca = pd.DataFrame(pca.transform(X_valid), columns=[f'PC{i+1}' for i in range(n_components)])\n",
        "  X_test_pca  = pd.DataFrame(pca.transform(X_test),  columns=[f'PC{i+1}' for i in range(n_components)])\n",
        "\n",
        "  # Add target back\n",
        "  df_train_pca = pd.concat([X_train_pca, y_train], axis=1)\n",
        "  df_valid_pca = pd.concat([X_valid_pca, y_valid], axis=1)\n",
        "  df_test_pca  = pd.concat([X_test_pca,  y_test],  axis=1)\n",
        "\n",
        "  return df_train_pca, df_valid_pca, df_test_pca"
      ],
      "metadata": {
        "id": "ywuE6x4uwK6m"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def numeric_conversion(df_orig):\n",
        "  df = df_orig.copy()\n",
        "\n",
        "  # Dropping 'Equinox' and 'Orbiting Body'\n",
        "  df.drop(columns=['Equinox', 'Orbiting Body'], errors='ignore', inplace=True)\n",
        "\n",
        "  # Converting COLUMNS to datetime\n",
        "  df['Close Approach Date'] = pd.to_datetime(df['Close Approach Date'])\n",
        "  df['Close Approach Year'] = df['Close Approach Date'].dt.year\n",
        "  df['Close Approach Month'] = df['Close Approach Date'].dt.month\n",
        "\n",
        "  df['Orbit Determination Date'] = pd.to_datetime(df['Orbit Determination Date'])\n",
        "  df['Orbit Determination Year'] = df['Orbit Determination Date'].dt.year\n",
        "  df['Orbit Determination Month'] = df['Orbit Determination Date'].dt.month\n",
        "\n",
        "  # Encoding 'Hazardous' column\n",
        "  df['Hazardous'] = df['Hazardous'].astype(int)\n",
        "\n",
        "  df = df.drop(columns=[\"Close Approach Date\", \"Orbit Determination Date\"])\n",
        "  return df"
      ],
      "metadata": {
        "id": "koAiTYCCgGyH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_date_features(df, month_cols, year_cols):\n",
        "  for col in month_cols:\n",
        "    if col in df.columns:\n",
        "      df[col] = (df[col] - 1) / (12 - 1)\n",
        "\n",
        "  for col in year_cols:\n",
        "    if col in df.columns:\n",
        "      df[col] = (df[col] - 1900) / (2100 - 1900)\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "dMGNUoqV7RCh"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_extraction(df_train, df_valid, df_test):\n",
        "  df_train = numeric_conversion(df_train)\n",
        "  df_valid = numeric_conversion(df_valid)\n",
        "  df_test = numeric_conversion(df_test)\n",
        "\n",
        "  month_cols = ['Close Approach Month', 'Orbit Determination Month']\n",
        "  year_cols = ['Close Approach Year', 'Orbit Determination Year']\n",
        "\n",
        "  # Normalize date features\n",
        "  df_train = normalize_date_features(df_train, month_cols, year_cols)\n",
        "  df_valid = normalize_date_features(df_valid, month_cols, year_cols)\n",
        "  df_test = normalize_date_features(df_test, month_cols, year_cols)\n",
        "\n",
        "  return df_train, df_valid, df_test"
      ],
      "metadata": {
        "id": "E4USGxvn7PPw"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_preprocessing(df):\n",
        "  df_train, df_temp = train_test_split(df, test_size=0.4, random_state=42)\n",
        "  df_valid, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\n",
        "  df_train.reset_index(drop=True, inplace=True)\n",
        "  df_valid.reset_index(drop=True, inplace=True)\n",
        "  df_test.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  no_outliers_train, no_outliers_valid, no_outliers_test = remove_outliers_zscore(df_train, df_valid, df_test)\n",
        "  no_highly_correlated_train, no_highly_correlated_valid, no_highly_correlated_test = remove_highly_correlated(no_outliers_train, no_outliers_valid, no_outliers_test)\n",
        "  normal_train, normal_valid, normal_test = normalize_minmax(no_highly_correlated_train, no_highly_correlated_valid, no_highly_correlated_test)\n",
        "  df_final_train, df_final_valid, df_final_test = feature_extraction(normal_train, normal_valid, normal_test)\n",
        "  return df_final_train, df_final_valid, df_final_test"
      ],
      "metadata": {
        "id": "DaeDAw9s6EIO"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_gausian(df):\n",
        "  df_train, df_temp = train_test_split(df, test_size=0.4, random_state=42)\n",
        "  df_valid, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\n",
        "  df_train.reset_index(drop=True, inplace=True)\n",
        "  df_valid.reset_index(drop=True, inplace=True)\n",
        "  df_test.reset_index(drop=True, inplace=True)\n",
        "\n",
        "\n",
        "  df_train, df_valid, df_test = remove_outliers_iqr(df_train, df_valid, df_test)\n",
        "  df_train, df_valid, df_test = feature_extraction(df_train, df_valid, df_test)\n",
        "  df_train, df_valid, df_test = standardize_data(df_train, df_valid, df_test)\n",
        "  df_train, df_valid, df_test = remove_highly_correlated(df_train, df_valid, df_test)\n",
        "\n",
        "  return df_train, df_valid, df_test"
      ],
      "metadata": {
        "id": "fj9V6mw8W7qO"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_knn1(df):\n",
        "  df_train, df_temp = train_test_split(df, test_size=0.4, random_state=42)\n",
        "  df_valid, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\n",
        "  df_train.reset_index(drop=True, inplace=True)\n",
        "  df_valid.reset_index(drop=True, inplace=True)\n",
        "  df_test.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  df_train, df_valid, df_test = remove_outliers_iqr(df_train, df_valid, df_test)\n",
        "  df_train, df_valid, df_test = label_encode_categorical(df_train, df_valid, df_test)\n",
        "  df_train, df_valid, df_test = standardize_data(df_train, df_valid, df_test)\n",
        "  df_train, df_valid, df_test = remove_highly_correlated(df_train, df_valid, df_test)\n",
        "  df_train, df_valid, df_test = perform_pca(df_train, df_valid, df_test)\n",
        "\n",
        "  return df_train, df_valid, df_test"
      ],
      "metadata": {
        "id": "K146NigQXfkY"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_knn2(df):\n",
        "  # Shows trade off between accuracy and recall\n",
        "  df_train, df_temp = train_test_split(df, test_size=0.4, random_state=42)\n",
        "  df_valid, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\n",
        "  df_train.reset_index(drop=True, inplace=True)\n",
        "  df_valid.reset_index(drop=True, inplace=True)\n",
        "  df_test.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  df_train, df_valid, df_test = feature_extraction(df_train, df_valid, df_test)\n",
        "  df_train, df_valid, df_test = standardize_data(df_train, df_valid, df_test)\n",
        "\n",
        "  return df_train, df_valid, df_test"
      ],
      "metadata": {
        "id": "FzhZZCArXfxb"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_logistic(df):\n",
        "  df_train, df_temp = train_test_split(df, test_size=0.4, random_state=42)\n",
        "  df_valid, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\n",
        "  df_train.reset_index(drop=True, inplace=True)\n",
        "  df_valid.reset_index(drop=True, inplace=True)\n",
        "  df_test.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  df_train, df_valid, df_test = remove_outliers_zscore(df_train, df_valid, df_test)\n",
        "  df_train, df_valid, df_test = feature_extraction(df_train, df_valid, df_test)\n",
        "  df_train, df_valid, df_test = standardize_data(df_train, df_valid, df_test)\n",
        "  df_train, df_valid, df_test = standardize_data(df_train, df_valid, df_test)\n",
        "\n",
        "  return df_train, df_valid, df_test\n",
        "\n"
      ],
      "metadata": {
        "id": "ajKiqiFLXf-C"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_perceptron(df):\n",
        "  # Not needed acctually\n",
        "  df_train, df_temp = train_test_split(df, test_size=0.4, random_state=42)\n",
        "  df_valid, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\n",
        "  df_train.reset_index(drop=True, inplace=True)\n",
        "  df_valid.reset_index(drop=True, inplace=True)\n",
        "  df_test.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  df_train, df_valid, df_test = remove_outliers_zscore(df_train, df_valid, df_test)\n",
        "  df_train, df_valid, df_test = label_encode_categorical(df_train, df_valid, df_test)\n",
        "  df_train, df_valid, df_test = standardize_data(df_train, df_valid, df_test)\n",
        "\n",
        "  return df_train, df_valid, df_test"
      ],
      "metadata": {
        "id": "MD3j6Tb5XgGk"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_random_forest(df):\n",
        "  df_train, df_temp = train_test_split(df, test_size=0.4, random_state=42)\n",
        "  df_valid, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\n",
        "  df_train.reset_index(drop=True, inplace=True)\n",
        "  df_valid.reset_index(drop=True, inplace=True)\n",
        "  df_test.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  df_train, df_valid, df_test = remove_outliers_iqr(df_train, df_valid, df_test)\n",
        "  df_train, df_valid, df_test = label_encode_categorical(df_train, df_valid, df_test)\n",
        "  df_train, df_valid, df_test = normalize_minmax(df_train, df_valid, df_test)\n",
        "  df_train, df_valid, df_test = remove_highly_correlated(df_train, df_valid, df_test)\n",
        "\n",
        "  return df_train, df_valid, df_test"
      ],
      "metadata": {
        "id": "DoJ_OMv_XgOM"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_svc1(df):\n",
        "  df_train, df_temp = train_test_split(df, test_size=0.4, random_state=42)\n",
        "  df_valid, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\n",
        "  df_train.reset_index(drop=True, inplace=True)\n",
        "  df_valid.reset_index(drop=True, inplace=True)\n",
        "  df_test.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  df_train, df_valid, df_test = label_encode_categorical(df_train, df_valid, df_test)\n",
        "  df_train, df_valid, df_test = standardize_data(df_train, df_valid, df_test)\n",
        "  return df_train, df_valid, df_test"
      ],
      "metadata": {
        "id": "s29WBl5hXgVY"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_svc2(df):\n",
        "  # Immune to outliers\n",
        "  df_train, df_temp = train_test_split(df, test_size=0.4, random_state=42)\n",
        "  df_valid, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\n",
        "  df_train.reset_index(drop=True, inplace=True)\n",
        "  df_valid.reset_index(drop=True, inplace=True)\n",
        "  df_test.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  df_train, df_valid, df_test = remove_outliers_zscore(df_train, df_valid, df_test)\n",
        "  df_train, df_valid, df_test = label_encode_categorical(df_train, df_valid, df_test)\n",
        "  df_train, df_valid, df_test = standardize_data(df_train, df_valid, df_test)\n",
        "  return df_train, df_valid, df_test"
      ],
      "metadata": {
        "id": "cOCjoPSzXgcm"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_svc_poly(df):\n",
        "  df_train, df_temp = train_test_split(df, test_size=0.4, random_state=42)\n",
        "  df_valid, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\n",
        "  df_train.reset_index(drop=True, inplace=True)\n",
        "  df_valid.reset_index(drop=True, inplace=True)\n",
        "  df_test.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  df_train, df_valid, df_test = remove_outliers_iqr(df_train, df_valid, df_test)\n",
        "  df_train, df_valid, df_test = feature_extraction(df_train, df_valid, df_test)\n",
        "  df_train, df_valid, df_test = normalize_minmax(df_train, df_valid, df_test)\n",
        "\n",
        "  return df_train, df_valid, df_test"
      ],
      "metadata": {
        "id": "wosVawLVXYpF"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_svc_rbf1(df):\n",
        "  df_train, df_temp = train_test_split(df, test_size=0.4, random_state=42)\n",
        "  df_valid, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\n",
        "  df_train.reset_index(drop=True, inplace=True)\n",
        "  df_valid.reset_index(drop=True, inplace=True)\n",
        "  df_test.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  df_train, df_valid, df_test = feature_extraction(df_train, df_valid, df_test)\n",
        "  df_train, df_valid, df_test = standardize_data(df_train, df_valid, df_test)\n",
        "  df_train, df_valid, df_test = remove_highly_correlated(df_train, df_valid, df_test)\n",
        "\n",
        "  return df_train, df_valid, df_test"
      ],
      "metadata": {
        "id": "e03U_iXwXY_A"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_svc_rbf2(df):\n",
        "  # Increasing recall might significantly drop accuracy.\n",
        "  df_train, df_temp = train_test_split(df, test_size=0.4, random_state=42)\n",
        "  df_valid, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\n",
        "  df_train.reset_index(drop=True, inplace=True)\n",
        "  df_valid.reset_index(drop=True, inplace=True)\n",
        "  df_test.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  df_train, df_valid, df_test = remove_outliers_iqr(df_train, df_valid, df_test)\n",
        "  df_train, df_valid, df_test = feature_extraction(df_train, df_valid, df_test)\n",
        "  df_train, df_valid, df_test = normalize_minmax(df_train, df_valid, df_test)\n",
        "\n",
        "  return df_train, df_valid, df_test"
      ],
      "metadata": {
        "id": "Gg8eRMa2gIxp"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # TESTING PURPOSE ONLY\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# df_testing_purpose = original_dataframe.copy()\n",
        "\n",
        "# df_train, df_temp = train_test_split(df_testing_purpose, test_size=0.4, random_state=42)\n",
        "# df_valid, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\n",
        "# df_train.reset_index(drop=True, inplace=True)\n",
        "# df_valid.reset_index(drop=True, inplace=True)\n",
        "# df_test.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# no_outliers_train, no_outliers_valid, no_outliers_test = remove_outliers_zscore(df_train, df_valid, df_test)\n",
        "# no_highly_correlated_train, no_highly_correlated_valid, no_highly_correlated_test = remove_highly_correlated(no_outliers_train, no_outliers_valid, no_outliers_test)\n",
        "# normal_train, normal_valid, normal_test = normalize_minmax(no_highly_correlated_train, no_highly_correlated_valid, no_highly_correlated_test)\n",
        "# df_final_train, df_final_valid, df_final_test = feature_extraction(normal_train, normal_valid, normal_test)\n",
        "\n",
        "\n",
        "# print(f\"Original data shape: {df_testing_purpose.shape}\")\n",
        "# print(\"\\n\")\n",
        "\n",
        "# print(f\"Train data shape: {df_train.shape}\")\n",
        "# print(f\"Validation data shape: {df_valid.shape}\")\n",
        "# print(f\"Test data shape: {df_test.shape}\")\n",
        "# print(\"\\n\")\n",
        "\n",
        "\n",
        "# print(f\"no_outliers_train shape: {no_outliers_train.shape}\")\n",
        "# print(f\"no_outliers_valid shape: {no_outliers_valid.shape}\")\n",
        "# print(f\"no_outliers_test shape: {no_outliers_test.shape}\")\n",
        "# print(\"\\n\")\n",
        "\n",
        "\n",
        "# print(f\"no_highly_correlated_train shape: {no_highly_correlated_train.shape}\")\n",
        "# print(f\"no_highly_correlated_valid shape: {no_highly_correlated_valid.shape}\")\n",
        "# print(f\"no_highly_correlated_test shape: {no_highly_correlated_test.shape}\")\n",
        "# print(\"\\n\")\n",
        "\n",
        "\n",
        "# print(f\"normal_train shape: {normal_train.shape}\")\n",
        "# print(f\"normal_valid shape: {normal_valid.shape}\")\n",
        "# print(f\"normal_test shape: {normal_test.shape}\")\n",
        "# print(\"\\n\")\n",
        "\n",
        "\n",
        "# print(f\"df_final_train shape: {df_final_train.shape}\")\n",
        "# print(f\"df_final_valid shape: {df_final_valid.shape}\")\n",
        "# print(f\"df_final_test shape: {df_final_test.shape}\")\n",
        "# print(\"\\n\")\n",
        "\n",
        "\n",
        "# print(f\" Hazardous count in no_outliers train : {no_outliers_train[no_outliers_train['Hazardous']==1].shape[0]}\")\n",
        "# print(f\" Hazardous count in df_final_train : {df_final_train[df_final_train['Hazardous']==1].shape[0]}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "KdCxsIQM-pbv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}