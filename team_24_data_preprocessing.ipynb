{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOnkPGw/yaDWtAm22sYzIAW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PriyanshuRao-code/AI-Lab-Project/blob/main/team_24_data_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TVbyxjtmp9hA",
        "outputId": "a1e2e987-9c6b-4971-9322-3dbe35ccd44c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Cloning the repository...\n",
            "Repository is ready to use at: /content/AI-Lab-Project\n"
          ]
        }
      ],
      "source": [
        "# Don't do anything here. It is just a setup.\n",
        "import os\n",
        "import sys\n",
        "\n",
        "repo_name = \"AI-Lab-Project\"\n",
        "repo_url = \"https://github.com/PriyanshuRao-code/AI-Lab-Project.git\"\n",
        "repo_path = f\"/content/{repo_name}\"\n",
        "\n",
        "if os.path.exists(repo_path):\n",
        "    print(\"Repository already exists at:\", repo_path)\n",
        "else:\n",
        "    print(\"ðŸš€ Cloning the repository...\")\n",
        "    os.system(f\"git clone {repo_url}\")\n",
        "\n",
        "os.chdir(repo_path)\n",
        "sys.path.append(repo_path)\n",
        "\n",
        "print(\"Repository is ready to use at:\", repo_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start your code from here"
      ],
      "metadata": {
        "id": "Q57cyNuL_uUt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import zscore\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "rpkn8eeUAHH9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_dataframe = pd.read_csv('24.csv')"
      ],
      "metadata": {
        "id": "lTfhW7smnger"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_outliers_iqr(df_train, df_valid, df_test):\n",
        "\n",
        "  df_train_numeric = df_train.select_dtypes(include=['number']).select_dtypes(exclude=['bool'])\n",
        "  df_train_non_numeric = df_train.select_dtypes(exclude=['number'])\n",
        "\n",
        "  Q1 = df_train_numeric.quantile(0.25)\n",
        "  Q3 = df_train_numeric.quantile(0.75)\n",
        "  IQR = Q3 - Q1\n",
        "\n",
        "  lower_bound = Q1 - 1.5 * IQR\n",
        "  upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "  # Removing outliers\n",
        "  df_iqr = df_train_numeric[~((df_train_numeric < lower_bound) | (df_train_numeric > upper_bound)).any(axis=1)]\n",
        "  df_train_cleaned = pd.concat([df_iqr, df_train_non_numeric.loc[df_iqr.index]], axis=1)\n",
        "\n",
        "  return df_train_cleaned, df_valid, df_test"
      ],
      "metadata": {
        "id": "zitP9in21c6O"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_outliers_zscore(df_train, df_valid, df_test, z_score_threshold = 3):\n",
        "\n",
        "  df_train_numeric = df_train.select_dtypes(include=['number']).select_dtypes(exclude=['bool'])\n",
        "  df_train_non_numeric = df_train.select_dtypes(exclude=['number'])\n",
        "\n",
        "  z_scores = df_train_numeric.apply(zscore)\n",
        "  df_z = df_train_numeric[(z_scores.abs() < z_score_threshold).all(axis=1)]  # Remove rows with Z-score >  z_score_threshold in any column\n",
        "  df_train_cleaned = pd.concat([df_z, df_train_non_numeric.loc[df_z.index]], axis=1)\n",
        "\n",
        "  return df_train_cleaned, df_valid, df_test"
      ],
      "metadata": {
        "id": "3riZW1Jo13LC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_highly_correlated(df_train, df_valid, df_test, target_col=\"Hazardous\", high_corr_threshold = 0.99):\n",
        "\n",
        "  df_train_numeric = df_train.select_dtypes(include=['number']).select_dtypes(exclude=['bool'])\n",
        "\n",
        "  target_series = None\n",
        "  if target_col in df_train_numeric.columns: # High correaltion with target column should not be dropped\n",
        "    target_series = df_train[target_col]\n",
        "    df_train_numeric = df_train_numeric.drop(columns=[target_col])\n",
        "\n",
        "  high_corr_pairs = set()\n",
        "  correlation_matrix = df_train_numeric.corr()\n",
        "\n",
        "  for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i): # Lower triangular matrix\n",
        "      if abs(correlation_matrix.iloc[i, j]) >= high_corr_threshold:\n",
        "        col1 = correlation_matrix.columns[i]\n",
        "        col2 = correlation_matrix.columns[j]\n",
        "        high_corr_pairs.add((col1, col2))\n",
        "\n",
        "  columns_to_drop = {col2 for col1, col2 in high_corr_pairs}\n",
        "\n",
        "  df_train_reduced = df_train.drop(columns=columns_to_drop)\n",
        "  df_valid_reduced = df_valid.drop(columns=columns_to_drop)\n",
        "  df_test_reduced = df_test.drop(columns=columns_to_drop)\n",
        "\n",
        "  if target_series is not None: # If present in df_numeric and is removed, has to be added again\n",
        "    df_train_reduced[target_col] = target_series\n",
        "\n",
        "  return df_train_reduced, df_train_reduced, df_test_reduced"
      ],
      "metadata": {
        "id": "7HbGOrEl7PI2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_minmax(df_train, df_test, df_valid, target_col=\"Hazardous\"):\n",
        "  scaler = MinMaxScaler()\n",
        "\n",
        "  df_train_numeric = df_train.select_dtypes(include=['number']).select_dtypes(exclude=['bool'])\n",
        "  df_test_numeric = df_test.select_dtypes(include=['number']).select_dtypes(exclude=['bool'])\n",
        "  df_valid_numeric = df_valid.select_dtypes(include=['number']).select_dtypes(exclude=['bool'])\n",
        "\n",
        "  # Handling target column\n",
        "  target_train, target_test, target_valid = None, None, None\n",
        "\n",
        "  if target_col in df_train_numeric.columns:\n",
        "    target_train = df_train[target_col]\n",
        "    df_train_numeric = df_train_numeric.drop(columns=[target_col])\n",
        "\n",
        "    target_test = df_test[target_col]\n",
        "    df_test_numeric = df_test_numeric.drop(columns=[target_col])\n",
        "\n",
        "    target_valid = df_valid[target_col]\n",
        "    df_valid_numeric = df_valid_numeric.drop(columns=[target_col])\n",
        "\n",
        "  # Normalization\n",
        "  df_train[df_train_numeric.columns] = scaler.fit_transform(df_train_numeric)\n",
        "  df_test[df_test_numeric.columns] = scaler.transform(df_test_numeric)\n",
        "  df_valid[df_valid_numeric.columns] = scaler.transform(df_valid_numeric)\n",
        "\n",
        "  # Again including the target_col\n",
        "  if target_train is not None:\n",
        "    df_train[target_col] = target_train\n",
        "  if target_test is not None:\n",
        "    df_test[target_col] = target_test\n",
        "  if target_valid is not None:\n",
        "    df_valid[target_col] = target_valid\n",
        "\n",
        "  return df_train, df_test, df_valid"
      ],
      "metadata": {
        "id": "f9XVR_A--2Rh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def standardize_data(df_train, df_test, df_valid, target_col=\"Hazardous\"):\n",
        "  scaler = StandardScaler()\n",
        "\n",
        "  df_train_numeric = df_train.select_dtypes(include=['number']).select_dtypes(exclude=['bool'])\n",
        "  df_test_numeric = df_test.select_dtypes(include=['number']).select_dtypes(exclude=['bool'])\n",
        "  df_valid_numeric = df_valid.select_dtypes(include=['number']).select_dtypes(exclude=['bool'])\n",
        "\n",
        "  # Handling target column\n",
        "  target_train, target_test, target_valid = None, None, None\n",
        "\n",
        "  if target_col in df_train_numeric.columns:\n",
        "    target_train = df_train[target_col]\n",
        "    df_train_numeric = df_train_numeric.drop(columns=[target_col])\n",
        "\n",
        "    target_test = df_test[target_col]\n",
        "    df_test_numeric = df_test_numeric.drop(columns=[target_col])\n",
        "\n",
        "    target_valid = df_valid[target_col]\n",
        "    df_valid_numeric = df_valid_numeric.drop(columns=[target_col])\n",
        "\n",
        "  # Standardization\n",
        "  df_train[df_train_numeric.columns] = scaler.fit_transform(df_train_numeric)\n",
        "  df_test[df_test_numeric.columns] = scaler.transform(df_test_numeric)\n",
        "  df_valid[df_valid_numeric.columns] = scaler.transform(df_valid_numeric)\n",
        "\n",
        "\n",
        "  # Again including the target_col\n",
        "  if target_train is not None:\n",
        "    df_train[target_col] = target_train\n",
        "  if target_test is not None:\n",
        "    df_test[target_col] = target_test\n",
        "  if target_valid is not None:\n",
        "    df_valid[target_col] = target_valid\n",
        "\n",
        "  return df_train, df_test, df_valid"
      ],
      "metadata": {
        "id": "1TEW4GRMRsj2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def label_encode_categorical(df_train, df_valid, df_test):\n",
        "\n",
        "  categorical_cols = df_train.select_dtypes(include=['object']).columns\n",
        "  label_encoders = {}\n",
        "\n",
        "\n",
        "  for col in categorical_cols:\n",
        "    label_encoders[col] = LabelEncoder()\n",
        "    df_train[col] = label_encoders[col].fit_transform(df_train[col])\n",
        "\n",
        "    df_valid[col] = df_valid[col].map(lambda x: label_encoders[col].transform([x])[0] if x in label_encoders[col].classes_ else -1)\n",
        "    df_valid[col] = df_valid[col].map(lambda x: label_encoders[col].transform([x])[0] if x in label_encoders[col].classes_ else -1)\n",
        "\n",
        "  return df_train, df_valid, df_test"
      ],
      "metadata": {
        "id": "5v2O0_pQSnPG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_high_corr_features(df_train, df_test, df_valid, target_col=\"Hazardous\", top_n=3):\n",
        "  df_train_numeric = df_train.select_dtypes(include=['number']).select_dtypes(exclude=['bool'])\n",
        "\n",
        "  # If target column is not in numerical form\n",
        "  if target_col not in df_train_numeric.columns:\n",
        "      raise ValueError(f\"Target column '{target_col}' must be numeric and present in the dataset.\")\n",
        "\n",
        "  corr_values = df_train_numeric.corr()[target_col].abs().sort_values(ascending=False)\n",
        "  selected_features = corr_values.drop(index=target_col).head(top_n).index.tolist()\n",
        "\n",
        "  # print(f\"Selected features based on correlation with '{target_col}': {selected_features}\")\n",
        "\n",
        "  return df_train[selected_features], df_test[selected_features], df_valid[selected_features]"
      ],
      "metadata": {
        "id": "MnF9g8lMT52q"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_pca(df_train, df_test, df_valid, n_components=3):\n",
        "  pca = PCA(n_components=n_components)\n",
        "\n",
        "  df_train_numeric = df_train.select_dtypes(include=['number'])\n",
        "  df_test_numeric = df_test.select_dtypes(include=['number'])\n",
        "  df_valid_numeric = df_valid.select_dtypes(include=['number'])\n",
        "\n",
        "  pca.fit(df_train_numeric)\n",
        "  df_train_pca = pca.transform(df_train_numeric)\n",
        "  df_test_pca = pca.transform(df_test_numeric)\n",
        "  df_valid_pca = pca.transform(df_valid_numeric)\n",
        "\n",
        "  pca_columns = [f'PC{i+1}' for i in range(n_components)]\n",
        "  df_train_pca = pd.DataFrame(df_train_pca, columns=pca_columns)\n",
        "  df_test_pca = pd.DataFrame(df_test_pca, columns=pca_columns)\n",
        "  df_valid_pca = pd.DataFrame(df_valid_pca, columns=pca_columns)\n",
        "\n",
        "  return df_train_pca, df_test_pca, df_valid_pca"
      ],
      "metadata": {
        "id": "ywuE6x4uwK6m"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def numeric_conversion(df_orig, one_hot_encode_month=False):\n",
        "  df = df_orig.copy()\n",
        "\n",
        "  # Dropping 'Equinox' and 'Orbiting Body'\n",
        "  df.drop(columns=['Equinox', 'Orbiting Body'], errors='ignore', inplace=True)\n",
        "\n",
        "  # Converting 'Close Approach Date' to datetime\n",
        "  df['Close Approach Date'] = pd.to_datetime(df['Close Approach Date'])\n",
        "  df['Close Approach Year'] = df['Close Approach Date'].dt.year\n",
        "  df['Close Approach Month'] = df['Close Approach Date'].dt.month\n",
        "\n",
        "  # Converting 'Orbit Determination Date' to datetime\n",
        "  df['Orbit Determination Date'] = pd.to_datetime(df['Orbit Determination Date'])\n",
        "  df['Orbit Determination Year'] = df['Orbit Determination Date'].dt.year\n",
        "  df['Orbit Determination Month'] = df['Orbit Determination Date'].dt.month\n",
        "\n",
        "\n",
        "  # Converting 'Epoch Date Close Approach' to datetime\n",
        "  df['Converted Date'] = df['Epoch Date Close Approach'].apply(\n",
        "    lambda x: datetime.utcfromtimestamp(x / 1000) if pd.notnull(x) else None\n",
        "  )\n",
        "\n",
        "  df['Epoch Close Approach Year'] = df['Converted Date'].dt.year\n",
        "  df['Epoch Close Approach Month'] = df['Converted Date'].dt.month\n",
        "\n",
        "\n",
        "  # Encoding 'Hazardous' column\n",
        "  df['Hazardous'] = df['Hazardous'].astype(int)\n",
        "\n",
        "  # One-Hot Encoding for months (if needed)\n",
        "  if one_hot_encode_month:\n",
        "    df = pd.get_dummies(df, columns=['Close Approach Month', 'Epoch Close Approach Month', 'Orbit Determination Month'], prefix=['Close Approach Month no.', 'Epoch Close Approach Month no.', 'Orbit Determination Month no.'], dtype=int)\n",
        "\n",
        "  df = df.drop(columns=[\"Converted Date\", \"Close Approach Date\", \"Epoch Date Close Approach\", \"Orbit Determination Date\"])\n",
        "  return df"
      ],
      "metadata": {
        "id": "koAiTYCCgGyH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_preprocessing(df):\n",
        "  encoded = numeric_conversion(df)\n",
        "  df_train, df_temp = train_test_split(encoded, test_size=0.4, random_state=42)\n",
        "  df_valid, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "  no_outliers_train, no_outliers_valid, no_outliers_test = remove_outliers_zscore(df_train, df_valid, df_test)\n",
        "  no_highly_correlated_train, no_highly_correlated_valid, no_highly_correlated_test = remove_highly_correlated(no_outliers_train, no_outliers_valid, no_outliers_test)\n",
        "  normal_train, normal_test, normal_valid = normalize_minmax(no_highly_correlated_train, no_highly_correlated_valid, no_highly_correlated_test)\n",
        "  normal_train.reset_index(drop=True, inplace=True)\n",
        "  normal_valid.reset_index(drop=True, inplace=True)\n",
        "  normal_test.reset_index(drop=True, inplace=True)\n",
        "  return normal_train, normal_valid, normal_test"
      ],
      "metadata": {
        "id": "DaeDAw9s6EIO"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}